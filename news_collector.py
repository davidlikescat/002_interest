#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
Google News ê²€ìƒ‰ + ê¸°ì‚¬ í¬ë¡¤ë§ + í‚¤ì›Œë“œ í•„í„°ë§ì„ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ í†µí•©
"""

import requests
import feedparser
import time
import logging
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin, urlparse
from bs4 import BeautifulSoup
import re

# í‚¤ì›Œë“œ ë§¤ë‹ˆì € import (ì„ íƒì )
try:
    from keyword_manager import KeywordManager
    KEYWORD_MANAGER_AVAILABLE = True
except ImportError:
    KEYWORD_MANAGER_AVAILABLE = False

logger = logging.getLogger(__name__)

class NewsCollector:
    """í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - Google News ê²€ìƒ‰ë¶€í„° í¬ë¡¤ë§ê¹Œì§€"""

    def __init__(self, max_articles=10, use_keyword_manager=True):
        self.max_articles = max_articles
        self.use_keyword_manager = use_keyword_manager and KEYWORD_MANAGER_AVAILABLE
        self.session = requests.Session()

        # ê³µí†µ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })

        # í‚¤ì›Œë“œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.keyword_manager = None
        if self.use_keyword_manager:
            try:
                self.keyword_manager = KeywordManager()
                logger.info("âœ… í‚¤ì›Œë“œ ë§¤ë‹ˆì € ì—°ê²° ì„±ê³µ")
            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_keyword_manager = False

        # AI ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ (Fallbackìš©)
        self.default_ai_keywords = [
            'ì¸ê³µì§€ëŠ¥', 'AI', 'ìƒì„±í˜•AI', 'ChatGPT', 'LLM', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹',
            'GPT', 'ììœ¨ì£¼í–‰', 'AIë°˜ë„ì²´', 'ë„¤ì´ë²„AI', 'ì¹´ì¹´ì˜¤AI', 'ì‚¼ì„±AI',
            'í´ë¡œë°”', 'ë°”ë“œ', 'Bard', 'êµ¬ê¸€AI', 'OpenAI', 'Claude'
        ]

        # í˜„ì¬ ì‚¬ìš©í•  í‚¤ì›Œë“œ ì„¤ì •
        self.ai_keywords = self._get_current_keywords()

        # ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ë³¸ë¬¸ ì„ íƒì
        self.content_selectors = {
            'aitimes.com': ['.article-content', '.news-content'],
            'zdnet.co.kr': ['.view_con', '.article_view'],
            'chosun.com': ['.news_text', '.article_view'],
            'dt.co.kr': ['.article-content', '.view-con'],
            'etnews.com': ['.article_txt', '.news-content'],
            'hankyung.com': ['.article-content'],
            'mk.co.kr': ['.news_detail_text']
        }

        # ì œê±°í•  ìš”ì†Œë“¤
        self.remove_selectors = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            '.advertisement', '.ad', '.ads', '.social-share',
            '.related-articles', '.comment', '.tag'
        ]

        # í†µê³„
        self.stats = {
            'searched_articles': 0,
            'crawled_articles': 0,
            'filtered_articles': 0,
            'failed_crawls': 0,
            'keyword_matches': {}
        }

    def _get_current_keywords(self) -> list:
        """í˜„ì¬ ì‚¬ìš©í•  í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
        if self.use_keyword_manager and self.keyword_manager:
            try:
                keywords = self.keyword_manager.get_search_keywords()
                logger.info(f"í‚¤ì›Œë“œ ë§¤ë‹ˆì €ì—ì„œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ë¡œë“œ")
                return keywords
            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ ë§¤ë‹ˆì €ì—ì„œ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        logger.info(f"ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {len(self.default_ai_keywords)}ê°œ")
        return self.default_ai_keywords

    def refresh_keywords(self) -> bool:
        """í‚¤ì›Œë“œ ìƒˆë¡œê³ ì¹¨"""
        try:
            if self.use_keyword_manager and self.keyword_manager:
                self.ai_keywords = self.keyword_manager.get_search_keywords(use_cache=False)
                logger.info(f"í‚¤ì›Œë“œ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ: {len(self.ai_keywords)}ê°œ")
                return True
            return False
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}")
            return False

    def get_keyword_info(self) -> dict:
        """í‚¤ì›Œë“œ ì •ë³´ ë°˜í™˜"""
        return {
            'total_keywords': len(self.ai_keywords),
            'keyword_manager_enabled': self.use_keyword_manager,
            'keywords_sample': self.ai_keywords[:10],
            'source': 'keyword_manager' if self.use_keyword_manager else 'default'
        }

    def collect_ai_news(self) -> list:
        """AI ë‰´ìŠ¤ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
        logger.info(f"AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {self.max_articles}ê°œ)")

        # 1ë‹¨ê³„: Google Newsì—ì„œ AI ë‰´ìŠ¤ ê²€ìƒ‰
        search_results = self._search_google_news()
        if not search_results:
            logger.warning("Google News ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        self.stats['searched_articles'] = len(search_results)
        logger.info(f"Google News ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë°œê²¬")

        # 2ë‹¨ê³„: ê° ê¸°ì‚¬ í¬ë¡¤ë§ ë° í•„í„°ë§
        collected_articles = []

        for i, article in enumerate(search_results[:self.max_articles], 1):
            try:
                logger.info(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({i}/{min(len(search_results), self.max_articles)}): {article['title'][:50]}...")

                # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
                content = self._crawl_article_content(article['url'])

                if content:
                    article['content'] = content
                    article['content_length'] = len(content)

                    # AI ê´€ë ¨ì„± í•„í„°ë§
                    if self._is_ai_related(article):
                        # í‚¤ì›Œë“œ ì¶”ì¶œ
                        article['found_keywords'] = self._extract_keywords(article)
                        collected_articles.append(article)
                        self.stats['filtered_articles'] += 1
                        logger.info(f"âœ… AI ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘: {article['title'][:50]}...")
                    else:
                        logger.info(f"â­ï¸ AI ë¬´ê´€ ê¸°ì‚¬ ìŠ¤í‚µ: {article['title'][:50]}...")

                    self.stats['crawled_articles'] += 1
                else:
                    # í¬ë¡¤ë§ ì‹¤íŒ¨í•´ë„ ìš”ì•½ìœ¼ë¡œ í¬í•¨
                    article['content'] = article.get('summary', '')
                    article['content_length'] = len(article['content'])
                    article['found_keywords'] = self._extract_keywords(article)

                    if self._is_ai_related(article):
                        collected_articles.append(article)
                        self.stats['filtered_articles'] += 1

                    self.stats['failed_crawls'] += 1

                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)

            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                self.stats['failed_crawls'] += 1
                continue

        # ìµœì‹ ìˆœ ì •ë ¬
        collected_articles.sort(key=lambda x: x['published'], reverse=True)

        logger.info(f"AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_articles)}ê°œ")
        self._print_statistics()

        return collected_articles

    def _search_google_news(self) -> list:
        """Google News RSSì—ì„œ AI ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            # AI í•µì‹¬ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            core_keywords = ['ì¸ê³µì§€ëŠ¥', 'AI', 'ìƒì„±í˜•AI', 'ChatGPT', 'LLM']
            search_query = ' OR '.join([f'"{kw}"' for kw in core_keywords])
            search_query += ' when:1d'  # ìµœê·¼ 1ì¼

            encoded_query = quote(search_query)
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"

            logger.info(f"Google News ê²€ìƒ‰: {rss_url}")

            # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
            response = self.session.get(rss_url, timeout=30)
            response.raise_for_status()

            # feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(response.content)

            if not feed.entries:
                return []

            articles = []
            for entry in feed.entries[:self.max_articles * 2]:  # ì—¬ìœ ë¶„ í™•ë³´
                try:
                    # ê¸°ì‚¬ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    article = {
                        'title': entry.title,
                        'url': self._extract_original_url(entry.link),
                        'summary': getattr(entry, 'summary', '')[:300],
                        'source': getattr(entry, 'source', {}).get('title', 'Unknown'),
                        'published': self._parse_published_time(entry),
                        'content': '',
                        'found_keywords': []
                    }
                    articles.append(article)

                except Exception as e:
                    logger.warning(f"RSS ì—”íŠ¸ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            return articles

        except Exception as e:
            logger.error(f"Google News ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _extract_original_url(self, google_news_url: str) -> str:
        """Google News URLì—ì„œ ì›ë³¸ ê¸°ì‚¬ URL ì¶”ì¶œ"""
        try:
            if 'news.google.com' not in google_news_url:
                return google_news_url

            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ê°€ê¸°
            response = self.session.head(google_news_url, allow_redirects=True, timeout=10)
            return response.url

        except Exception:
            return google_news_url

    def _parse_published_time(self, entry) -> datetime:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë°œí–‰ ì‹œê°„ íŒŒì‹±"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6])
        except Exception:
            pass
        return datetime.now()

    def _crawl_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=20)
            response.raise_for_status()

            # ì¸ì½”ë”© ì„¤ì •
            if response.encoding.lower() in ['iso-8859-1', 'ascii']:
                response.encoding = 'utf-8'

            soup = BeautifulSoup(response.content, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_main_content(soup, url)
            return self._clean_text(content)

        except Exception as e:
            logger.warning(f"í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return ""

    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> str:
        """ë©”ì¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        domain = urlparse(url).netloc.lower()

        # ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ì„ íƒì ì‚¬ìš©
        selectors = []
        for site_domain, site_selectors in self.content_selectors.items():
            if site_domain in domain:
                selectors = site_selectors
                break

        # ê¸°ë³¸ ì„ íƒì
        if not selectors:
            selectors = [
                'article', '.article-content', '.news-content', '.post-content',
                '.entry-content', '.article-body', '.content', '.main-content'
            ]

        # ì„ íƒìë³„ë¡œ ì‹œë„
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for remove_selector in self.remove_selectors:
                    for tag in element.select(remove_selector):
                        tag.decompose()

                text = element.get_text(separator=' ', strip=True)
                if len(text) > 200:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    return text

        # í´ë°±: p íƒœê·¸ë“¤ ìˆ˜ì§‘
        paragraphs = soup.find_all('p')
        if paragraphs:
            return ' '.join([p.get_text(strip=True) for p in paragraphs])

        return ""

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""

        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        import html
        text = html.unescape(text)

        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = text.replace('\u200b', '').replace('\ufeff', '')

        return text.strip()

    def _is_ai_related(self, article: dict) -> bool:
        """ê¸°ì‚¬ê°€ AI ê´€ë ¨ì¸ì§€ í™•ì¸"""
        text_to_check = f"{article['title']} {article['content']} {article['summary']}".lower()

        # AI í‚¤ì›Œë“œ ì¡´ì¬ í™•ì¸
        for keyword in self.ai_keywords:
            if keyword.lower() in text_to_check:
                return True

        return False

    def _extract_keywords(self, article: dict) -> list:
        """ê¸°ì‚¬ì—ì„œ ë°œê²¬ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í†µê³„ ì—…ë°ì´íŠ¸"""
        text_to_check = f"{article['title']} {article['content']}".lower()
        found_keywords = []

        for keyword in self.ai_keywords:
            if keyword.lower() in text_to_check:
                found_keywords.append(keyword)

                # í‚¤ì›Œë“œ ë§¤ì¹˜ í†µê³„ ì—…ë°ì´íŠ¸
                if keyword not in self.stats['keyword_matches']:
                    self.stats['keyword_matches'][keyword] = 0
                self.stats['keyword_matches'][keyword] += 1

                # í‚¤ì›Œë“œ ë§¤ë‹ˆì €ì— ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                if self.use_keyword_manager and self.keyword_manager:
                    try:
                        self.keyword_manager.update_usage(keyword, 1)
                    except Exception as e:
                        logger.warning(f"í‚¤ì›Œë“œ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({keyword}): {e}")

        return found_keywords[:5]  # ìµœëŒ€ 5ê°œ

    def _print_statistics(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘ í†µê³„:")
        print(f"  â€¢ ê²€ìƒ‰ëœ ê¸°ì‚¬: {self.stats['searched_articles']}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì„±ê³µ: {self.stats['crawled_articles']}ê°œ")
        print(f"  â€¢ AI ê´€ë ¨ í•„í„°ë§: {self.stats['filtered_articles']}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì‹¤íŒ¨: {self.stats['failed_crawls']}ê°œ")

        if self.stats['searched_articles'] > 0:
            success_rate = (self.stats['crawled_articles'] / self.stats['searched_articles']) * 100
            print(f"  â€¢ í¬ë¡¤ë§ ì„±ê³µë¥ : {success_rate:.1f}%")

        # í‚¤ì›Œë“œ ë§¤ë‹ˆì € ì •ë³´
        print(f"\nğŸ”‘ í‚¤ì›Œë“œ ì •ë³´:")
        keyword_info = self.get_keyword_info()
        print(f"  â€¢ ì´ í‚¤ì›Œë“œ: {keyword_info['total_keywords']}ê°œ")
        print(f"  â€¢ í‚¤ì›Œë“œ ì†ŒìŠ¤: {keyword_info['source']}")

        if self.stats['keyword_matches']:
            print(f"  â€¢ ë§¤ì¹˜ëœ í‚¤ì›Œë“œ: {len(self.stats['keyword_matches'])}ê°œ")
            top_keywords = sorted(
                self.stats['keyword_matches'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            for keyword, count in top_keywords:
                print(f"    - {keyword}: {count}íšŒ")
        else:
            print(f"  â€¢ ë§¤ì¹˜ëœ í‚¤ì›Œë“œ: ì—†ìŒ")

    def get_statistics(self) -> dict:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return self.stats.copy()


def test_collector():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ ë§¤ë‹ˆì € í†µí•©)")
    print("=" * 50)

    # í‚¤ì›Œë“œ ë§¤ë‹ˆì € ì—°ë™ í…ŒìŠ¤íŠ¸
    collector = NewsCollector(max_articles=3, use_keyword_manager=True)

    # í‚¤ì›Œë“œ ì •ë³´ ì¶œë ¥
    keyword_info = collector.get_keyword_info()
    print(f"ğŸ”‘ í‚¤ì›Œë“œ ë§¤ë‹ˆì €: {'âœ… ì—°ê²°ë¨' if keyword_info['keyword_manager_enabled'] else 'âŒ ë¯¸ì—°ê²°'}")
    print(f"ğŸ“ ì‚¬ìš© í‚¤ì›Œë“œ: {keyword_info['total_keywords']}ê°œ ({keyword_info['source']})")
    print(f"ğŸ“‹ í‚¤ì›Œë“œ ì˜ˆì‹œ: {', '.join(keyword_info['keywords_sample'][:5])}")

    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    articles = collector.collect_ai_news()

    if articles:
        print(f"\nâœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ:")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title']}")
            print(f"   ì¶œì²˜: {article['source']}")
            print(f"   ë°œí–‰: {article['published'].strftime('%Y-%m-%d %H:%M')}")
            print(f"   í‚¤ì›Œë“œ: {', '.join(article['found_keywords'])}")
            print(f"   ë³¸ë¬¸: {len(article['content'])}ì")
            print()

        # í‚¤ì›Œë“œ ë§¤ë‹ˆì € í†µê³„ (ê°€ëŠ¥í•œ ê²½ìš°)
        if collector.use_keyword_manager and collector.keyword_manager:
            try:
                stats = collector.keyword_manager.get_statistics(days=1)
                print(f"\nğŸ“ˆ í‚¤ì›Œë“œ ë§¤ë‹ˆì € í†µê³„ (ìµœê·¼ 1ì¼):")
                print(f"  â€¢ ì´ ê²€ìƒ‰: {stats['total_searches']}íšŒ")
                print(f"  â€¢ ë§¤ì¹˜ëœ ê¸°ì‚¬: {stats['total_articles']}ê°œ")
                print(f"  â€¢ í™œì„± í‚¤ì›Œë“œ: {stats['unique_keywords']}ê°œ")
            except Exception as e:
                logger.warning(f"í‚¤ì›Œë“œ ë§¤ë‹ˆì € í†µê³„ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    else:
        print("âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")


if __name__ == "__main__":
    test_collector()